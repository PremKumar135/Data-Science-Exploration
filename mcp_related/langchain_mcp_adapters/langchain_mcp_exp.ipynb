{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using Langchain MCP Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Single MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructuredTool(name='add', description='Add two numbers', args_schema={'properties': {'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}, 'required': ['a', 'b'], 'title': 'addArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x10a331300>), StructuredTool(name='multiply', description='Multiply two numbers', args_schema={'properties': {'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}, 'required': ['a', 'b'], 'title': 'multiplyArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x10a331580>)]\n",
      "{'messages': [HumanMessage(content=\"what's (3 + 5) x 12?\", additional_kwargs={}, response_metadata={}, id='cc3afdaa-1202-4acf-a4c9-d37102e5bf08'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_TC3kRN2gtiSu2CF3N97XE2bk', 'function': {'arguments': '{\"a\": 3, \"b\": 5}', 'name': 'add'}, 'type': 'function'}, {'id': 'call_2EbGODVEkltoWOzeYe2W7qzU', 'function': {'arguments': '{\"a\": 12, \"b\": 1}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 77, 'total_tokens': 128, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_96c46af214', 'id': 'chatcmpl-BTwCTO9Bixy7S9yWYuCaViXo5mw6A', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--923dde39-210a-4ef8-8b46-3a551b9cb432-0', tool_calls=[{'name': 'add', 'args': {'a': 3, 'b': 5}, 'id': 'call_TC3kRN2gtiSu2CF3N97XE2bk', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 12, 'b': 1}, 'id': 'call_2EbGODVEkltoWOzeYe2W7qzU', 'type': 'tool_call'}], usage_metadata={'input_tokens': 77, 'output_tokens': 51, 'total_tokens': 128, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='8', name='add', id='6047b8c5-c348-4114-9938-398f6aa94404', tool_call_id='call_TC3kRN2gtiSu2CF3N97XE2bk'), ToolMessage(content='12', name='multiply', id='3d2ee67c-5fe2-42cd-9adf-4ef83d844f2f', tool_call_id='call_2EbGODVEkltoWOzeYe2W7qzU'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_fEwyIQCaJTFYYab9whxUYjdl', 'function': {'arguments': '{\"a\":8,\"b\":12}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 143, 'total_tokens': 161, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_96c46af214', 'id': 'chatcmpl-BTwCUwvXy7BbpBPeYKVlR8B8nJsgp', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--957a191d-8017-447b-8dd4-84e9c62ce6ce-0', tool_calls=[{'name': 'multiply', 'args': {'a': 8, 'b': 12}, 'id': 'call_fEwyIQCaJTFYYab9whxUYjdl', 'type': 'tool_call'}], usage_metadata={'input_tokens': 143, 'output_tokens': 18, 'total_tokens': 161, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='96', name='multiply', id='b40a417e-81ef-4f7f-8032-3441a481fef4', tool_call_id='call_fEwyIQCaJTFYYab9whxUYjdl'), AIMessage(content='The result of \\\\((3 + 5) \\\\times 12\\\\) is 96.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 168, 'total_tokens': 190, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_96c46af214', 'id': 'chatcmpl-BTwCVDVNulPzubYpLrEb6JAOsOSsr', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--8a87f202-3a9a-45bc-8385-63f410ed5688-0', usage_metadata={'input_tokens': 168, 'output_tokens': 22, 'total_tokens': 190, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "# Create server parameters for stdio connection\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "server_params = StdioServerParameters(command=\"python\", args=[\"./mcp_server_math.py\"],)\n",
    "\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        # Initialize the connection\n",
    "        await session.initialize()\n",
    "\n",
    "        # Get tools\n",
    "        tools = await load_mcp_tools(session)\n",
    "        print(tools)\n",
    "\n",
    "        # Create and run the agent\n",
    "        agent = create_react_agent(model, tools)\n",
    "        agent_response = await agent.ainvoke({\"messages\": \"what's (3 + 5) x 12?\"})\n",
    "        print(agent_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of \\((3 + 5) \\times 12\\) is 96.\n"
     ]
    }
   ],
   "source": [
    "print(agent_response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Multiple servers at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of \\((3 + 5) \\times 12\\) is 96.\n",
      "The weather in NYC is always sunny!\n"
     ]
    }
   ],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "config = {\n",
    "        \"math\":{\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"./mcp_server_math.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "            },\n",
    "        \"weather\": {\n",
    "            # make sure you start your weather server on port 8000\n",
    "            \"url\": \"http://localhost:8000/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "            }\n",
    "    }\n",
    "\n",
    "async with MultiServerMCPClient(config) as client:\n",
    "    agent = create_react_agent(model, client.get_tools())\n",
    "    math_response = await agent.ainvoke({\"messages\": \"what's (3 + 5) x 12?\"})\n",
    "    weather_response = await agent.ainvoke({\"messages\": \"what is the weather in nyc?\"})\n",
    "\n",
    "print(agent_response['messages'][-1].content)\n",
    "print(weather_response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Trying only with config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'command' parameter is required for stdio connection",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprebuilt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_react_agent\n\u001b[32m      6\u001b[39m config = {\n\u001b[32m      7\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33mmcpServers\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfilesystem\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m   }\n\u001b[32m     18\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m MultiServerMCPClient(config) \u001b[38;5;28;01mas\u001b[39;00m client:\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(client.get_tools())\n\u001b[32m     23\u001b[39m     agent = create_react_agent(model, client.get_tools())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my_pc/programming/code_git_not_completed_yet/Data-science-exploration/mcp_related/.venv/lib/python3.12/site-packages/langchain_mcp_adapters/client.py:358\u001b[39m, in \u001b[36mMultiServerMCPClient.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    356\u001b[39m     connections = \u001b[38;5;28mself\u001b[39m.connections \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m server_name, connection \u001b[38;5;129;01min\u001b[39;00m connections.items():\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.connect_to_server(server_name, **connection)\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my_pc/programming/code_git_not_completed_yet/Data-science-exploration/mcp_related/.venv/lib/python3.12/site-packages/langchain_mcp_adapters/client.py:181\u001b[39m, in \u001b[36mMultiServerMCPClient.connect_to_server\u001b[39m\u001b[34m(self, server_name, transport, **kwargs)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m transport == \u001b[33m\"\u001b[39m\u001b[33mstdio\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcommand\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcommand\u001b[39m\u001b[33m'\u001b[39m\u001b[33m parameter is required for stdio connection\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33margs\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m    183\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33margs\u001b[39m\u001b[33m'\u001b[39m\u001b[33m parameter is required for stdio connection\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: 'command' parameter is required for stdio connection"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "config = {\n",
    "  \"mcpServers\": {\n",
    "    \"filesystem\": {\n",
    "      \"command\": \"npx\",\n",
    "      \"args\": [\n",
    "        \"-y\",\n",
    "        \"@modelcontextprotocol/server-filesystem\",\n",
    "        \"/Users/username/Desktop\",\n",
    "        \"/path/to/other/allowed/dir\"\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "async with MultiServerMCPClient(config) as client:\n",
    "    print(client.get_tools())\n",
    "    agent = create_react_agent(model, client.get_tools())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_mcp",
   "language": "python",
   "name": "venv_mcp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
